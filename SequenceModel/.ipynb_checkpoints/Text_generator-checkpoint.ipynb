{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the library for training data from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137629 chars, 81 unique\n"
     ]
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    Prepare a python library, which uses number to represent characters\n",
    "    ---\n",
    "    Parameters\n",
    "    ---\n",
    "    vocab_size: number of uniquie characters\n",
    "    char_to_ix: python library converting character to number\n",
    "    ix_to_char: python library convering number back to character\n",
    "    \"\"\"\n",
    "    data = open('kafka.txt', 'r').read()\n",
    "\n",
    "    chars = list(set(data)) \n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    print ('data has %d chars, %d unique' % (data_size, vocab_size))\n",
    "    \n",
    "    # character to index\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "    \n",
    "    # index to character\n",
    "    ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "    \n",
    "    return(data, vocab_size, char_to_ix, ix_to_char)\n",
    "    \n",
    "data, vocab_size, char_to_ix, ix_to_char = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define gradient clipping to avoid gradient expose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]\n",
    "    for gradient in [dWaa, dWax, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Create the one-hot vector x for the first character (initializing the sequence generation). \n",
    "    x = np.zeros((vocab_size, 1)) # xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    # Initialize a_prev as zeros\n",
    "    a_prev = np.zeros((n_a, 1)) # hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate \n",
    "    indices = []\n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "     \n",
    "    # At each time-step, sample a character from a probability distribution and append its index to \"indices\".     \n",
    "    counter = 0 # set a counter so that if we reach 50 characters to pevent an infinite loop    \n",
    "    idx = -1 \n",
    "    # if we hit a new line then stop    \n",
    "    while (idx !=  char_to_ix['\\n'] and counter!= 50):\n",
    "        #  Forward propagate x and also update a_prev\n",
    "        a, p, _ = rnn_cell_forward(x, a_prev, parameters) # p shape (n_y, m) in this case n_y = n_x\n",
    "        # fix random seed\n",
    "        np.random.seed(counter + seed)\n",
    "        # Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(list(range(vocab_size)), p = p.ravel())\n",
    "        indices.append(idx) # append the index of the characters\n",
    "\n",
    "        #  Overwrite the input character as the one corresponding to the sampled index\n",
    "        x = np.zeros_like(x)\n",
    "        x[idx] = 1\n",
    "        a_prev = a\n",
    "        \n",
    "        counter +=1\n",
    "        \n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model, setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(vocab_size):\n",
    "    \"\"\"\n",
    "    parameters -- python dictionary containing:\n",
    "                    Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                    Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                    Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                    ba --  Bias numpy array of shape (n_a, 1)\n",
    "                    by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    \"\"\"\n",
    "    seq_length = 25\n",
    "    learning_rate = 1e-1\n",
    "    \n",
    "    # hidden layer size\n",
    "    n_a = 100\n",
    "    # input_size \n",
    "    n_x= vocab_size\n",
    "    # output size\n",
    "    n_y = vocab_size\n",
    "\n",
    "    Waa = np.random.randn(n_a, n_a)* 0.01\n",
    "    Wax = np.random.randn(n_a, n_x)* 0.01 \n",
    "    Wya = np.random.randn(n_y, n_a)* 0.01\n",
    "    ba = np.zeros((n_a, 1))\n",
    "    by = np.zeros((n_y, 1))\n",
    "                  \n",
    "    parameters = {'Waa': Waa, 'Wax': Wax, 'Wya': Wya, 'ba': ba, 'by': by}\n",
    "    return (seq_length, learning_rate, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define rnn cell foward with loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward_loss(xt, a_prev, parameters): # one RNN cell with n_a neurons\n",
    "    \"\"\"\n",
    "    m: number of examples\n",
    "    n_x: number of features for the input\n",
    "    n_a: hidden node\n",
    "    n_y: number of features for the output\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    p -- normalized probability distribution (n_y, m)\n",
    "    \"\"\"\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba) # shape (n_a, m)\n",
    "  \n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by) # shape(n_y, m)\n",
    "\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    p = np.exp(yt_pred) / np.sum(np.exp(yt_pred))\n",
    "    \n",
    "    return (a_next, yt_pred, cache, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward_loss(x, y, parameters): # RNN chains\n",
    "    \"\"\"\n",
    "    T_x: number of time step\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    y -- Target data for every time-step, of shape (n_y, m, T_x).\n",
    "\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    #init loss as 0\n",
    "    loss = 0\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and Wy\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    # chain of RNN\n",
    "    cache_store = []\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "\n",
    "    for t in range(T_x):\n",
    "        if t == 0:\n",
    "            a_prev = np.zeros((n_a,m)) # initial the hideen state\n",
    "        a_next, y_pred[:,:,t], cache, p = rnn_cell_forward_loss(x[:,:,t], a_prev, parameters)\n",
    "        caches.append(cache)\n",
    "        a_prev, _, xt, parameters = cache\n",
    "        a[:,:,t] = a_next\n",
    "        \n",
    "        loss += -np.log(p*[y[:,:,t],0]) #  cross-entropy loss                                                                                                                     \n",
    "\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return(a, y_pred, caches, loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length, learning_rate, parameters = model_init(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev, parameters):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "      inputs,targets are both list of integers.                                                                                                                                                   \n",
    "      hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "      returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "    \"\"\"\n",
    "    #store our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    \n",
    "#       #init loss as 0\n",
    "#     loss = 0\n",
    "#       # forward pass                                                                                                                                                                              \n",
    "#     for t in range(len(inputs)):\n",
    "#         xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "#         xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "#         hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "#         ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "#         ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "#         loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)   \n",
    "    hs, y_pred, caches, loss = rnn_forward_loss(inputs, targerts, parameters)\n",
    "    gradients =  rnn_backward(loss, caches):\n",
    "\n",
    "      # backward pass: compute gradients going backwards    \n",
    "      #initalize vectors for gradient values for each set of weights \n",
    "    \n",
    "#     dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "#     dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "#     dhnext = np.zeros_like(hs[0])\n",
    "#     for t in reversed(range(len(inputs))):\n",
    "#         #output probabilities\n",
    "#         dy = np.copy(ps[t])\n",
    "#         #derive our first gradient\n",
    "#         dy[targets[t]] -= 1 # backprop into y  \n",
    "#         #compute output gradient -  output times hidden states transpose\n",
    "#         #When we apply the transpose weight matrix,  \n",
    "#         #we can think intuitively of this as moving the error backward\n",
    "#         #through the network, giving us some sort of measure of the error \n",
    "#         #at the output of the lth layer. \n",
    "#         #output gradient\n",
    "#         dWhy += np.dot(dy, hs[t].T)\n",
    "#         #derivative of output bias\n",
    "#         dby += dy\n",
    "#         #backpropagate!\n",
    "#         dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "#         dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "#         dbh += dhraw #derivative of hidden bias\n",
    "#         dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "#         dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "#         dhnext = np.dot(Whh.T, dhraw) \n",
    "#     for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "#         np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "#     return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    return (loss, gradients, hs[len(inputs)-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
